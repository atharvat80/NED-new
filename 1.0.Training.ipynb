{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.gbrt import GBRT\n",
    "from src.transformer import GBRT_TRF\n",
    "from src.gbrt import get_edit_dist, get_entity_prior, get_max_prior_prob, get_prior_prob\n",
    "from src.utils import cosine_similarity, get_document, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = os.path.join(os.getcwd(), 'embeddings')\n",
    "\n",
    "features = ['priorProb', 'entityPrior', 'maxPriorProb', 'numCands',\n",
    "            'editDist', 'mentionIsCand', 'mentionInCand', 'isStartorEnd',\n",
    "            'contextSim', 'coherence', 'rank']\n",
    "\n",
    "embs  = [\"wiki2vec_w10_100d.pkl\", \"wiki2vec_w10_300d.pkl\", \n",
    "         \"word2vec-google-news-300\", \"glove-wiki-gigaword-300\",\n",
    "         \"fasttext-wiki-news-subwords-300\"]\n",
    "\n",
    "entities = load_pickle('./data/aida/entities.pkl')\n",
    "entities_filtered = load_pickle('./data/aida/entities_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(model):\n",
    "    dfs = []\n",
    "    for i in tqdm(range(1, 1163)):\n",
    "        data = pd.read_csv(f'./data/aida/candidates/{i}.csv')\n",
    "        mentions = data['mention'].unique()\n",
    "        candidates = data['candidate'].unique()\n",
    "        max_prob = get_max_prior_prob(mentions, candidates)\n",
    "        \n",
    "        # Base features\n",
    "        data['priorProb'] = [get_prior_prob(i[1], i[2])\n",
    "                            for i in data[['candidate', 'mention']].itertuples()]\n",
    "        data['entityPrior'] = data['candidate'].map(get_entity_prior)\n",
    "        data['maxPriorProb'] = data['candidate'].map(max_prob)\n",
    "        \n",
    "        # String similarity features\n",
    "        ment_normalised = data['mention'].map(lambda x: x.lower())\n",
    "        cand_normalised = data['candidate'].map(lambda x: x.lower().replace('_', ' '))\n",
    "        ment_cand = list(zip(ment_normalised, cand_normalised))\n",
    "        data['editDist'] = [get_edit_dist(m, c) for m, c in ment_cand]\n",
    "        data['mentionIsCand'] = [int(m == c) for m, c in ment_cand]\n",
    "        data['mentionInCand'] = [int(m in c) for m, c in ment_cand]\n",
    "        data['isStartorEnd'] = [int(c.startswith(m) or c.endswith(m)) for m, c in ment_cand]\n",
    "\n",
    "        # Context based features\n",
    "        # Context similarity \n",
    "        context_emb = model.encode_sentence(get_document(i))\n",
    "        data['contextSim'] = data['candidate'].map(lambda x: cosine_similarity(model.encode_entity(x), context_emb))\n",
    "        # Coherence score\n",
    "        unamb_entities = data[data['priorProb'] >= 0.95]['candidate'].unique()\n",
    "        context_ent_emb = model.encode_context_entities(unamb_entities)\n",
    "        data['coherence'] = data['candidate'].map(lambda x: cosine_similarity(model.encode_entity(x), context_ent_emb))\n",
    "\n",
    "        # Add ground truth\n",
    "        data['y'] = (data['candidate'] == data['tag']).map(int)\n",
    "        dfs.append(data)\n",
    "\n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    #  add rank\n",
    "    dfs = []\n",
    "    while X.shape[0] != 0:\n",
    "        n = X.iloc[0]['numCands']\n",
    "        temp = X.head(n).copy()\n",
    "        temp['score'] = temp.contextSim\t+ temp.coherence\n",
    "        temp = temp.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        temp['rank'] = temp.index + 1\n",
    "        X = X.iloc[n:]\n",
    "        dfs.append(temp)\n",
    "        \n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "    return X.drop(columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7687a58e8e746fba6d3cf57735827c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7c3b27fa434f0e90e9d13dd51813e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a2c7c0624a4f8685eedbd6b74addab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8124e5223e47eb896314c4780acc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a9aaa907d94d93acac50174b68c7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for emb in embs:\n",
    "    model = GBRT(os.path.join(EMB_PATH, emb), cased = 'word2vec' in emb)\n",
    "    model.entity_desc_dict = entities_filtered\n",
    "    train_df = generate_train_data(model)\n",
    "    train_df.to_csv(f\"./data/GBRT/{emb}_train.csv\", index=False)\n",
    "    model = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eccd5679d224017b082d2e1b7f35f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GBRT_TRF()\n",
    "model.entity_desc_dict = entities\n",
    "train_df = generate_train_data(model)\n",
    "train_df.to_csv(f\"./data/GBRT/TRF_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GBRT (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(f\"./data/GBRT/{embs[0]}_train.csv\")\n",
    "y_train = X['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline GBRT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0722           29.33m\n",
      "         2           0.0703           30.08m\n",
      "         3           0.0685           29.21m\n",
      "         4           0.0668           31.40m\n",
      "         5           0.0651           32.69m\n",
      "         6           0.0635           34.41m\n",
      "         7           0.0619           36.15m\n",
      "         8           0.0604           34.92m\n",
      "         9           0.0590           35.16m\n",
      "        10           0.0577           35.27m\n",
      "        20           0.0466           36.53m\n",
      "        30           0.0391           35.23m\n",
      "        40           0.0341           34.07m\n",
      "        50           0.0306           32.36m\n",
      "        60           0.0283           31.48m\n",
      "        70           0.0267           31.18m\n",
      "        80           0.0255           31.00m\n",
      "        90           0.0247           30.73m\n",
      "       100           0.0241           30.56m\n",
      "       200           0.0219           30.97m\n",
      "       300           0.0212           31.31m\n",
      "       400           0.0206           31.19m\n",
      "       500           0.0201           29.55m\n",
      "       600           0.0197           28.40m\n",
      "       700           0.0193           27.44m\n",
      "       800           0.0189           26.66m\n",
      "       900           0.0186           26.01m\n",
      "      1000           0.0182           25.40m\n",
      "      2000           0.0156           21.62m\n",
      "      3000           0.0139           18.60m\n",
      "      4000           0.0126           15.88m\n",
      "      5000           0.0116           13.22m\n",
      "      6000           0.0108           10.55m\n",
      "      7000           0.0101            7.91m\n",
      "      8000           0.0096            5.28m\n",
      "      9000           0.0091            2.64m\n",
      "     10000           0.0087            0.00s\n"
     ]
    }
   ],
   "source": [
    "model.fit(X[features[:4]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0721           42.77m\n",
      "         2           0.0702           44.71m\n",
      "         3           0.0683           43.13m\n",
      "         4           0.0665           46.09m\n",
      "         5           0.0648           50.56m\n",
      "         6           0.0632           51.01m\n",
      "         7           0.0616           49.95m\n",
      "         8           0.0600           50.11m\n",
      "         9           0.0586           49.33m\n",
      "        10           0.0572           49.51m\n",
      "        20           0.0457           42.75m\n",
      "        30           0.0380           41.20m\n",
      "        40           0.0328           40.10m\n",
      "        50           0.0292           39.42m\n",
      "        60           0.0268           38.68m\n",
      "        70           0.0251           38.43m\n",
      "        80           0.0239           38.09m\n",
      "        90           0.0230           37.95m\n",
      "       100           0.0224           37.67m\n",
      "       200           0.0200           36.14m\n",
      "       300           0.0192           35.35m\n",
      "       400           0.0186           34.84m\n",
      "       500           0.0181           34.52m\n",
      "       600           0.0176           34.12m\n",
      "       700           0.0171           33.86m\n",
      "       800           0.0167           33.54m\n",
      "       900           0.0164           33.17m\n",
      "      1000           0.0160           32.83m\n",
      "      2000           0.0138           28.74m\n",
      "      3000           0.0125           25.07m\n",
      "      4000           0.0114           21.30m\n",
      "      5000           0.0106           17.68m\n",
      "      6000           0.0099           14.04m\n",
      "      7000           0.0093           10.51m\n",
      "      8000           0.0088            6.97m\n",
      "      9000           0.0084            3.47m\n",
      "     10000           0.0081            0.00s\n"
     ]
    }
   ],
   "source": [
    "model.fit(X[features[:8]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/string_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           60.83m\n",
      "         2           0.0700           60.82m\n",
      "         3           0.0681           61.57m\n",
      "         4           0.0662           67.20m\n",
      "         5           0.0644           65.52m\n",
      "         6           0.0626           63.92m\n",
      "         7           0.0610           62.74m\n",
      "         8           0.0594           61.55m\n",
      "         9           0.0578           60.66m\n",
      "        10           0.0564           60.03m\n",
      "        20           0.0443           55.39m\n",
      "        30           0.0361           54.13m\n",
      "        40           0.0305           53.65m\n",
      "        50           0.0265           53.31m\n",
      "        60           0.0237           52.88m\n",
      "        70           0.0218           52.55m\n",
      "        80           0.0204           52.31m\n",
      "        90           0.0194           51.99m\n",
      "       100           0.0185           51.63m\n",
      "       200           0.0158           49.69m\n",
      "       300           0.0149           48.68m\n",
      "       400           0.0142           48.18m\n",
      "       500           0.0137           47.56m\n",
      "       600           0.0133           46.99m\n",
      "       700           0.0130           46.54m\n",
      "       800           0.0128           47.44m\n",
      "       900           0.0126           46.37m\n",
      "      1000           0.0123           45.44m\n",
      "      2000           0.0105           39.86m\n",
      "      3000           0.0095           34.46m\n",
      "      4000           0.0086           29.33m\n",
      "      5000           0.0080           24.36m\n",
      "      6000           0.0075           19.67m\n",
      "      7000           0.0070           14.86m\n",
      "      8000           0.0066            9.96m\n",
      "      9000           0.0062            5.00m\n",
      "     10000           0.0059            0.00s\n"
     ]
    }
   ],
   "source": [
    "model.fit(X[features[:9]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/context.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0718           72.31m\n",
      "         2           0.0696           71.79m\n",
      "         3           0.0675           71.80m\n",
      "         4           0.0654           76.97m\n",
      "         5           0.0634           80.97m\n",
      "         6           0.0615           80.77m\n",
      "         7           0.0597           81.86m\n",
      "         8           0.0579           82.52m\n",
      "         9           0.0562           82.05m\n",
      "        10           0.0546           81.81m\n",
      "        20           0.0415           81.99m\n",
      "        30           0.0326           79.43m\n",
      "        40           0.0265           78.02m\n",
      "        50           0.0223           77.20m\n",
      "        60           0.0194           76.51m\n",
      "        70           0.0174           76.04m\n",
      "        80           0.0159           75.83m\n",
      "        90           0.0149           75.46m\n",
      "       100           0.0142           75.20m\n",
      "       200           0.0119           72.38m\n",
      "       300           0.0113           71.01m\n",
      "       400           0.0108           70.35m\n",
      "       500           0.0105           69.33m\n",
      "       600           0.0103           69.05m\n",
      "       700           0.0100           68.49m\n",
      "       800           0.0098           67.56m\n",
      "       900           0.0096           66.77m\n",
      "      1000           0.0094           66.00m\n",
      "      2000           0.0082           59.39m\n",
      "      3000           0.0074           52.42m\n",
      "      4000           0.0068           45.18m\n",
      "      5000           0.0063           37.64m\n",
      "      6000           0.0058           30.19m\n",
      "      7000           0.0055           22.63m\n",
      "      8000           0.0051           14.99m\n",
      "      9000           0.0049            7.46m\n",
      "     10000           0.0046            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719           77.16m\n",
      "         2           0.0698           85.82m\n",
      "         3           0.0677           85.29m\n",
      "         4           0.0657           84.04m\n",
      "         5           0.0638           83.34m\n",
      "         6           0.0620           82.33m\n",
      "         7           0.0603           81.51m\n",
      "         8           0.0586           81.27m\n",
      "         9           0.0569           81.99m\n",
      "        10           0.0554           82.83m\n",
      "        20           0.0427           78.51m\n",
      "        30           0.0341           77.12m\n",
      "        40           0.0282           76.46m\n",
      "        50           0.0242           75.81m\n",
      "        60           0.0214           75.56m\n",
      "        70           0.0194           75.11m\n",
      "        80           0.0179           74.88m\n",
      "        90           0.0169           74.70m\n",
      "       100           0.0161           74.48m\n",
      "       200           0.0136           72.94m\n",
      "       300           0.0128           71.60m\n",
      "       400           0.0123           70.91m\n",
      "       500           0.0119           70.16m\n",
      "       600           0.0116           69.29m\n",
      "       700           0.0113           68.43m\n",
      "       800           0.0111           67.78m\n",
      "       900           0.0109           67.09m\n",
      "      1000           0.0107           66.27m\n",
      "      2000           0.0092           58.61m\n",
      "      3000           0.0083           50.91m\n",
      "      4000           0.0077           45.17m\n",
      "      5000           0.0071           38.27m\n",
      "      6000           0.0066           30.56m\n",
      "      7000           0.0061           22.91m\n",
      "      8000           0.0057           15.30m\n",
      "      9000           0.0054            7.77m\n",
      "     10000           0.0051            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720          108.72m\n",
      "         2           0.0699          108.64m\n",
      "         3           0.0679          113.06m\n",
      "         4           0.0660          115.18m\n",
      "         5           0.0641          117.60m\n",
      "         6           0.0623          116.03m\n",
      "         7           0.0606          114.69m\n",
      "         8           0.0590          114.61m\n",
      "         9           0.0574          115.41m\n",
      "        10           0.0559          115.19m\n",
      "        20           0.0436          110.53m\n",
      "        30           0.0352          110.03m\n",
      "        40           0.0295          109.32m\n",
      "        50           0.0255          108.53m\n",
      "        60           0.0228          108.30m\n",
      "        70           0.0209          108.77m\n",
      "        80           0.0195          109.33m\n",
      "        90           0.0185          108.28m\n",
      "       100           0.0178          107.16m\n",
      "       200           0.0151          103.29m\n",
      "       300           0.0143           97.34m\n",
      "       400           0.0138           92.40m\n",
      "       500           0.0134           89.73m\n",
      "       600           0.0130           86.11m\n",
      "       700           0.0127           82.68m\n",
      "       800           0.0124           81.59m\n",
      "       900           0.0122           79.37m\n",
      "      1000           0.0119           78.02m\n",
      "      2000           0.0103           64.55m\n",
      "      3000           0.0092           60.20m\n",
      "      4000           0.0085           53.44m\n",
      "      5000           0.0079           44.78m\n",
      "      6000           0.0073           35.72m\n",
      "      7000           0.0069           26.68m\n",
      "      8000           0.0065           17.82m\n",
      "      9000           0.0061            8.70m\n",
      "     10000           0.0058            0.00s\n"
     ]
    }
   ],
   "source": [
    "for emb in embs[2:]:\n",
    "    X = pd.read_csv(f\"./data/GBRT/{emb}_train.csv\")\n",
    "    model.fit(X[features].to_numpy(), X['y'].to_numpy())\n",
    "    save_model(model, f\"./data/GBRT/{emb}_trained.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           68.00m\n",
      "         2           0.0699           64.11m\n",
      "         3           0.0679           63.64m\n",
      "         4           0.0660           62.79m\n",
      "         5           0.0642           62.77m\n",
      "         6           0.0624           62.56m\n",
      "         7           0.0607           62.01m\n",
      "         8           0.0591           62.11m\n",
      "         9           0.0575           61.89m\n",
      "        10           0.0561           61.70m\n",
      "        20           0.0439           60.86m\n",
      "        30           0.0357           60.63m\n",
      "        40           0.0301           60.35m\n",
      "        50           0.0262           60.30m\n",
      "        60           0.0235           60.17m\n",
      "        70           0.0215           60.06m\n",
      "        80           0.0201           60.00m\n",
      "        90           0.0191           59.96m\n",
      "       100           0.0184           59.89m\n",
      "       200           0.0157           61.72m\n",
      "       300           0.0149           61.41m\n",
      "       400           0.0143           60.91m\n",
      "       500           0.0138           60.20m\n",
      "       600           0.0135           59.70m\n",
      "       700           0.0131           59.05m\n",
      "       800           0.0129           58.47m\n",
      "       900           0.0126           59.20m\n",
      "      1000           0.0124           59.39m\n",
      "      2000           0.0108           53.32m\n",
      "      3000           0.0097           46.38m\n",
      "      4000           0.0088           39.72m\n",
      "      5000           0.0081           33.05m\n",
      "      6000           0.0076           26.45m\n",
      "      7000           0.0070           19.85m\n",
      "      8000           0.0066           13.23m\n",
      "      9000           0.0062            6.61m\n",
      "     10000           0.0059            0.00s\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(f\"./data/GBRT/TRF_train.csv\")\n",
    "model.fit(X[features].to_numpy(), X['y'].to_numpy())\n",
    "save_model(model, f\"./data/GBRT/TRF_trained.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3c83394ae731fac5cbf34b5abe7ebcd59fb96b846f104eed2689aeb9dd8ae81"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
