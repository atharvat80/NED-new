{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.x_train=torch.tensor(X, dtype=torch.float32)\n",
    "    self.y_train=torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_in_features=11, dropout=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_in_features, 1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "features = ['priorProb', 'entityPrior', 'maxPriorProb', 'numCands',\n",
    "            'editDist', 'mentionIsCand', 'mentionInCand', 'isStartorEnd',\n",
    "            'contextSim', 'coherence', 'rank']\n",
    "\n",
    "X = pd.read_csv(f\"./data/GBRT/wiki2vec_w10_100d.pkl_train.csv\")\n",
    "dataset = Dataset(X[features].values, X['y'].values)\n",
    "trainloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    mlp.train()\n",
    "    # Run the training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Set current loss value\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Get and prepare inputs\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "            targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record loss\n",
    "            num_batches += 1\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"[{epoch+1}/{epochs}] Average Loss: {'%.5f'%(total_loss/num_batches)}\")\n",
    "    mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] Average Loss: 0.10137\n",
      "[2/500] Average Loss: 0.02501\n",
      "[3/500] Average Loss: 0.02249\n",
      "[4/500] Average Loss: 0.02143\n",
      "[5/500] Average Loss: 0.02022\n",
      "[6/500] Average Loss: 0.01915\n",
      "[7/500] Average Loss: 0.01850\n",
      "[8/500] Average Loss: 0.01763\n",
      "[9/500] Average Loss: 0.01712\n",
      "[10/500] Average Loss: 0.01677\n",
      "[11/500] Average Loss: 0.01626\n",
      "[12/500] Average Loss: 0.01596\n",
      "[13/500] Average Loss: 0.01571\n",
      "[14/500] Average Loss: 0.01550\n",
      "[15/500] Average Loss: 0.01508\n",
      "[16/500] Average Loss: 0.01498\n",
      "[17/500] Average Loss: 0.01475\n",
      "[18/500] Average Loss: 0.01457\n",
      "[19/500] Average Loss: 0.01433\n",
      "[20/500] Average Loss: 0.01422\n",
      "[21/500] Average Loss: 0.01421\n",
      "[22/500] Average Loss: 0.01398\n",
      "[23/500] Average Loss: 0.01395\n",
      "[24/500] Average Loss: 0.01377\n",
      "[25/500] Average Loss: 0.01373\n",
      "[26/500] Average Loss: 0.01364\n",
      "[27/500] Average Loss: 0.01359\n",
      "[28/500] Average Loss: 0.01356\n",
      "[29/500] Average Loss: 0.01341\n",
      "[30/500] Average Loss: 0.01337\n",
      "[31/500] Average Loss: 0.01330\n",
      "[32/500] Average Loss: 0.01331\n",
      "[33/500] Average Loss: 0.01331\n",
      "[34/500] Average Loss: 0.01310\n",
      "[35/500] Average Loss: 0.01312\n",
      "[36/500] Average Loss: 0.01310\n",
      "[37/500] Average Loss: 0.01299\n",
      "[38/500] Average Loss: 0.01305\n",
      "[39/500] Average Loss: 0.01302\n",
      "[40/500] Average Loss: 0.01295\n",
      "[41/500] Average Loss: 0.01305\n",
      "[42/500] Average Loss: 0.01302\n",
      "[43/500] Average Loss: 0.01289\n",
      "[44/500] Average Loss: 0.01282\n",
      "[45/500] Average Loss: 0.01297\n",
      "[46/500] Average Loss: 0.01292\n",
      "[47/500] Average Loss: 0.01281\n",
      "[48/500] Average Loss: 0.01276\n",
      "[49/500] Average Loss: 0.01272\n",
      "[50/500] Average Loss: 0.01276\n",
      "[51/500] Average Loss: 0.01273\n",
      "[52/500] Average Loss: 0.01278\n",
      "[53/500] Average Loss: 0.01265\n",
      "[54/500] Average Loss: 0.01267\n",
      "[55/500] Average Loss: 0.01266\n",
      "[56/500] Average Loss: 0.01261\n",
      "[57/500] Average Loss: 0.01260\n",
      "[58/500] Average Loss: 0.01258\n",
      "[59/500] Average Loss: 0.01265\n",
      "[60/500] Average Loss: 0.01263\n",
      "[61/500] Average Loss: 0.01261\n",
      "[62/500] Average Loss: 0.01258\n",
      "[63/500] Average Loss: 0.01263\n",
      "[64/500] Average Loss: 0.01252\n",
      "[65/500] Average Loss: 0.01255\n",
      "[66/500] Average Loss: 0.01246\n",
      "[67/500] Average Loss: 0.01245\n",
      "[68/500] Average Loss: 0.01243\n",
      "[69/500] Average Loss: 0.01249\n",
      "[70/500] Average Loss: 0.01240\n",
      "[71/500] Average Loss: 0.01242\n",
      "[72/500] Average Loss: 0.01236\n",
      "[73/500] Average Loss: 0.01249\n",
      "[74/500] Average Loss: 0.01247\n",
      "[75/500] Average Loss: 0.01233\n",
      "[76/500] Average Loss: 0.01242\n",
      "[77/500] Average Loss: 0.01229\n",
      "[78/500] Average Loss: 0.01228\n",
      "[79/500] Average Loss: 0.01233\n",
      "[80/500] Average Loss: 0.01232\n",
      "[81/500] Average Loss: 0.01234\n",
      "[82/500] Average Loss: 0.01225\n",
      "[83/500] Average Loss: 0.01233\n",
      "[84/500] Average Loss: 0.01222\n",
      "[85/500] Average Loss: 0.01220\n",
      "[86/500] Average Loss: 0.01222\n",
      "[87/500] Average Loss: 0.01230\n",
      "[88/500] Average Loss: 0.01220\n",
      "[89/500] Average Loss: 0.01226\n",
      "[90/500] Average Loss: 0.01227\n",
      "[91/500] Average Loss: 0.01218\n",
      "[92/500] Average Loss: 0.01216\n",
      "[93/500] Average Loss: 0.01231\n",
      "[94/500] Average Loss: 0.01224\n",
      "[95/500] Average Loss: 0.01221\n",
      "[96/500] Average Loss: 0.01215\n",
      "[97/500] Average Loss: 0.01217\n",
      "[98/500] Average Loss: 0.01209\n",
      "[99/500] Average Loss: 0.01208\n",
      "[100/500] Average Loss: 0.01205\n",
      "[101/500] Average Loss: 0.01213\n",
      "[102/500] Average Loss: 0.01203\n",
      "[103/500] Average Loss: 0.01214\n",
      "[104/500] Average Loss: 0.01206\n",
      "[105/500] Average Loss: 0.01215\n",
      "[106/500] Average Loss: 0.01203\n",
      "[107/500] Average Loss: 0.01212\n",
      "[108/500] Average Loss: 0.01199\n",
      "[109/500] Average Loss: 0.01201\n",
      "[110/500] Average Loss: 0.01211\n",
      "[111/500] Average Loss: 0.01209\n",
      "[112/500] Average Loss: 0.01207\n",
      "[113/500] Average Loss: 0.01208\n",
      "[114/500] Average Loss: 0.01209\n",
      "[115/500] Average Loss: 0.01199\n",
      "[116/500] Average Loss: 0.01197\n",
      "[117/500] Average Loss: 0.01198\n",
      "[118/500] Average Loss: 0.01199\n",
      "[119/500] Average Loss: 0.01193\n",
      "[120/500] Average Loss: 0.01190\n",
      "[121/500] Average Loss: 0.01202\n",
      "[122/500] Average Loss: 0.01199\n",
      "[123/500] Average Loss: 0.01200\n",
      "[124/500] Average Loss: 0.01196\n",
      "[125/500] Average Loss: 0.01195\n",
      "[126/500] Average Loss: 0.01186\n",
      "[127/500] Average Loss: 0.01199\n",
      "[128/500] Average Loss: 0.01192\n",
      "[129/500] Average Loss: 0.01188\n",
      "[130/500] Average Loss: 0.01196\n",
      "[131/500] Average Loss: 0.01183\n",
      "[132/500] Average Loss: 0.01186\n",
      "[133/500] Average Loss: 0.01187\n",
      "[134/500] Average Loss: 0.01194\n",
      "[135/500] Average Loss: 0.01190\n",
      "[136/500] Average Loss: 0.01179\n",
      "[137/500] Average Loss: 0.01191\n",
      "[138/500] Average Loss: 0.01175\n",
      "[139/500] Average Loss: 0.01193\n",
      "[140/500] Average Loss: 0.01174\n",
      "[141/500] Average Loss: 0.01171\n",
      "[142/500] Average Loss: 0.01189\n",
      "[143/500] Average Loss: 0.01170\n",
      "[144/500] Average Loss: 0.01191\n",
      "[145/500] Average Loss: 0.01168\n",
      "[146/500] Average Loss: 0.01182\n",
      "[147/500] Average Loss: 0.01184\n",
      "[148/500] Average Loss: 0.01186\n",
      "[149/500] Average Loss: 0.01183\n",
      "[150/500] Average Loss: 0.01182\n",
      "[151/500] Average Loss: 0.01187\n",
      "[152/500] Average Loss: 0.01176\n",
      "[153/500] Average Loss: 0.01172\n",
      "[154/500] Average Loss: 0.01181\n",
      "[155/500] Average Loss: 0.01176\n",
      "[156/500] Average Loss: 0.01173\n",
      "[157/500] Average Loss: 0.01169\n",
      "[158/500] Average Loss: 0.01171\n",
      "[159/500] Average Loss: 0.01177\n",
      "[160/500] Average Loss: 0.01188\n",
      "[161/500] Average Loss: 0.01175\n",
      "[162/500] Average Loss: 0.01172\n",
      "[163/500] Average Loss: 0.01167\n",
      "[164/500] Average Loss: 0.01166\n",
      "[165/500] Average Loss: 0.01181\n",
      "[166/500] Average Loss: 0.01172\n",
      "[167/500] Average Loss: 0.01171\n",
      "[168/500] Average Loss: 0.01168\n",
      "[169/500] Average Loss: 0.01168\n",
      "[170/500] Average Loss: 0.01183\n",
      "[171/500] Average Loss: 0.01176\n",
      "[172/500] Average Loss: 0.01174\n",
      "[173/500] Average Loss: 0.01163\n",
      "[174/500] Average Loss: 0.01167\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(500)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(mlp.state_dict(), './data/NN_ranker.pt')\n",
    "mlp.load_state_dict(torch.load('./data/NN_ranker.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966212809085846\n"
     ]
    }
   ],
   "source": [
    "x = X[features].values[-1].tolist()\n",
    "with torch.no_grad():\n",
    "    print(mlp(torch.Tensor(x).to(device)).item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45de09fcf3bd2b79f524a4bd8f7de9946892c46adb40c2fbf980884cdce71d86"
  },
  "kernelspec": {
   "display_name": "NEDpy38kernel",
   "language": "python",
   "name": "nedpy38kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
