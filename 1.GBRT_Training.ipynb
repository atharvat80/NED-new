{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GBRT data files...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from src.gbrt import *\n",
    "from src.utils import cos_sim, get_document, load_json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = \"C:\\\\Personal Files\\\\NED-using-KG\\\\embeddings\\\\\"\n",
    "\n",
    "features = ['priorProb', 'entityPrior', 'maxPriorProb', 'numCands',\n",
    "            'editDist', 'mentionIsCand', 'mentionInCand', 'isStartorEnd',\n",
    "            'contextSim', 'coherence', 'rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(model):\n",
    "    dfs = []\n",
    "    for i in tqdm(range(1, 1163)):\n",
    "        data = pd.read_csv(f'./data/aida/candidates/{i}.csv')\n",
    "        mentions = data['mention'].unique()\n",
    "        candidates = data['candidate'].unique()\n",
    "        max_prob = get_max_prior_prob(mentions, candidates)\n",
    "        \n",
    "        # Base features\n",
    "        data['priorProb'] = [get_prior_prob(i[1], i[2])\n",
    "                            for i in data[['candidate', 'mention']].itertuples()]\n",
    "        data['entityPrior'] = data['candidate'].map(get_entity_prior)\n",
    "        data['maxPriorProb'] = data['candidate'].map(max_prob)\n",
    "        \n",
    "        # String similarity features\n",
    "        ment_normalised = data['mention'].map(lambda x: x.lower())\n",
    "        cand_normalised = data['candidate'].map(lambda x: x.lower().replace('_', ' '))\n",
    "        ment_cand = list(zip(ment_normalised, cand_normalised))\n",
    "        data['editDist'] = [get_edit_dist(m, c) for m, c in ment_cand]\n",
    "        data['mentionIsCand'] = [m == c for m, c in ment_cand]\n",
    "        data['mentionInCand'] = [m in c for m, c in ment_cand]\n",
    "        data['isStartorEnd'] = [c.startswith(m) or c.endswith(m) for m, c in ment_cand]\n",
    "\n",
    "        # Context based features\n",
    "        # Context similarity \n",
    "        context_emb = model.encode_sentence(get_document(i))\n",
    "        data['contextSim'] = data['candidate'].map(\n",
    "            lambda x: cos_sim(model.encode_entity(x), context_emb))\n",
    "        # Coherence score\n",
    "        unamb_entities = data[data['priorProb'] >= 0.95]['candidate'].unique()\n",
    "        context_ent_emb = model.encode_context_entities(unamb_entities)\n",
    "        data['coherence'] = data['candidate'].map(\n",
    "            lambda x: cos_sim(model.encode_entity(x), context_ent_emb))\n",
    "\n",
    "        # Add ground truth\n",
    "        data['y'] = (data['candidate'] == data['tag']).map(int)\n",
    "        dfs.append(data)\n",
    "\n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    #  add rank\n",
    "    dfs = []\n",
    "    while X.shape[0] != 0:\n",
    "        n = X.iloc[0]['numCands']\n",
    "        temp = X.head(n).copy()\n",
    "        temp['score'] = temp.contextSim\t+ temp.coherence\n",
    "        temp = temp.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        temp['rank'] = temp.index + 1\n",
    "        X = X.iloc[n:]\n",
    "        dfs.append(temp)\n",
    "        \n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "    return X.drop(columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555f073a41be4f1cadff0ef4244ac0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29f20e3b8a6412eb69d3e6fd98ec21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embs = [f\"wiki2vec_w10_{i}d.pkl\" for i in [100, 300]]\n",
    "for emb in embs:\n",
    "    model = GBRT(EMB_PATH + emb)\n",
    "    train_df = generate_train_data(model)\n",
    "    train_df.to_csv(f\"./data/GBRT/{emb}_train.csv\", index=False)\n",
    "    model = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de2ef946c0440fba0e45da635b44727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb = 'glove-wiki-gigaword-300'\n",
    "model = GBRT2(EMB_PATH + emb)\n",
    "model.entity_desc_dict = load_json(\"C:\\\\Personal Files\\\\NED-using-KG\\\\data\\\\aida\\\\entities.json\")\n",
    "train_df = generate_train_data(model)\n",
    "train_df.to_csv(f\"./data/GBRT/{emb}_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GBRT (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"./data/GBRT/wiki2vec_w10_300d.pkl_train.csv\")\n",
    "X_train = X.drop(columns=['mention', 'candidate', 'tag', 'y'])\n",
    "y_train = X['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[features[:4]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[features[:8]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/string_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[features[:9]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/context.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[features].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/coherence.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719           80.88m\n",
      "         2           0.0698           81.45m\n",
      "         3           0.0677           81.86m\n",
      "         4           0.0657           93.86m\n",
      "         5           0.0638           96.06m\n",
      "         6           0.0620           95.96m\n",
      "         7           0.0603           93.36m\n",
      "         8           0.0585           90.51m\n",
      "         9           0.0569           87.68m\n",
      "        10           0.0553           90.43m\n",
      "        20           0.0425           83.46m\n",
      "        30           0.0336           80.44m\n",
      "        40           0.0276           77.08m\n",
      "        50           0.0233           75.67m\n",
      "        60           0.0203           76.29m\n",
      "        70           0.0182           73.99m\n",
      "        80           0.0167           72.14m\n",
      "        90           0.0156           71.61m\n",
      "       100           0.0149           71.00m\n",
      "       200           0.0122           78.74m\n",
      "       300           0.0115           76.20m\n",
      "       400           0.0110           76.74m\n",
      "       500           0.0107           74.92m\n",
      "       600           0.0105           73.23m\n",
      "       700           0.0102           71.89m\n",
      "       800           0.0100           70.69m\n",
      "       900           0.0098           69.64m\n",
      "      1000           0.0096           68.28m\n",
      "      2000           0.0084           55.57m\n",
      "      3000           0.0076           50.13m\n",
      "      4000           0.0069           41.78m\n",
      "      5000           0.0064           34.13m\n",
      "      6000           0.0060           26.94m\n",
      "      7000           0.0056           20.21m\n",
      "      8000           0.0052           13.31m\n",
      "      9000           0.0048            6.58m\n",
      "     10000           0.0046            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719           62.66m\n",
      "         2           0.0698           68.20m\n",
      "         3           0.0677           70.40m\n",
      "         4           0.0657           70.18m\n",
      "         5           0.0638           69.97m\n",
      "         6           0.0620           69.34m\n",
      "         7           0.0602           68.64m\n",
      "         8           0.0586           69.83m\n",
      "         9           0.0570           70.70m\n",
      "        10           0.0555           70.43m\n",
      "        20           0.0430           66.68m\n",
      "        30           0.0343           65.71m\n",
      "        40           0.0282           65.29m\n",
      "        50           0.0241           64.97m\n",
      "        60           0.0212           64.16m\n",
      "        70           0.0191           64.01m\n",
      "        80           0.0177           63.93m\n",
      "        90           0.0167           63.99m\n",
      "       100           0.0159           63.56m\n",
      "       200           0.0131           62.23m\n",
      "       300           0.0123           61.03m\n",
      "       400           0.0118           59.55m\n",
      "       500           0.0115           58.67m\n",
      "       600           0.0112           57.95m\n",
      "       700           0.0109           57.20m\n",
      "       800           0.0107           56.52m\n",
      "       900           0.0105           55.85m\n",
      "      1000           0.0103           55.15m\n",
      "      2000           0.0089           54.12m\n",
      "      3000           0.0080           49.50m\n",
      "      4000           0.0073           41.49m\n",
      "      5000           0.0067           33.93m\n",
      "      6000           0.0062           26.75m\n",
      "      7000           0.0059           19.82m\n",
      "      8000           0.0055           13.15m\n",
      "      9000           0.0052            6.53m\n",
      "     10000           0.0049            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           89.00m\n",
      "         2           0.0699           85.40m\n",
      "         3           0.0679           91.42m\n",
      "         4           0.0660           89.22m\n",
      "         5           0.0641           86.74m\n",
      "         6           0.0623           84.53m\n",
      "         7           0.0606           81.85m\n",
      "         8           0.0590           79.83m\n",
      "         9           0.0574           78.95m\n",
      "        10           0.0559           78.21m\n",
      "        20           0.0435           74.63m\n",
      "        30           0.0351           73.52m\n",
      "        40           0.0294           72.03m\n",
      "        50           0.0254           70.96m\n",
      "        60           0.0225           70.66m\n",
      "        70           0.0206           70.50m\n",
      "        80           0.0193           70.24m\n",
      "        90           0.0183           70.06m\n",
      "       100           0.0176           69.68m\n",
      "       200           0.0152           67.68m\n",
      "       300           0.0145           69.60m\n",
      "       400           0.0140           68.45m\n",
      "       500           0.0136           72.74m\n",
      "       600           0.0133           75.58m\n",
      "       700           0.0130           77.38m\n",
      "       800           0.0127           78.32m\n",
      "       900           0.0125           78.75m\n",
      "      1000           0.0123           79.27m\n",
      "      2000           0.0106           67.97m\n",
      "      3000           0.0096           54.12m\n",
      "      4000           0.0087           44.37m\n",
      "      5000           0.0080           35.93m\n",
      "      6000           0.0074           28.18m\n",
      "      7000           0.0069           20.81m\n",
      "      8000           0.0064           13.70m\n",
      "      9000           0.0061            6.79m\n",
      "     10000           0.0057            0.00s\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "\n",
    "embs = [f\"wiki2vec_w10_{i}d.pkl\" for i in [100, 300]] + ['glove-wiki-gigaword-300']\n",
    "for emb in embs:\n",
    "    X = pd.read_csv(f\"./data/GBRT/{emb}_train.csv\")\n",
    "    X_train = X[features].to_numpy()\n",
    "    y_train = X['y'].to_numpy()\n",
    "    model.fit(X_train, y_train)\n",
    "    save_model(model, f\"./data/GBRT/{emb}_trained.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3c83394ae731fac5cbf34b5abe7ebcd59fb96b846f104eed2689aeb9dd8ae81"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NED-using-KG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
