{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from src.GBRT import GBRT\n",
    "from src.GBRT.utils import *\n",
    "from src.utils import cos_sim, get_document\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMB_PATH = \"C:\\\\Personal Files\\\\NED-using-KG\\\\embeddings\\\\wiki2vec_w10_100d.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "gbrt = GBRT(EMB_PATH)\n",
    "for i in tqdm(range(1, 1163)):\n",
    "    data = pd.read_csv(f'./data/aida/candidates/{i}.csv')\n",
    "    mentions = data['mention'].unique()\n",
    "    candidates = data['candidate'].unique()\n",
    "    max_prob = get_max_prior_prob(mentions, candidates)\n",
    "    \n",
    "    # Base features\n",
    "    data['priorProb'] = [get_prior_prob(i[1], i[2])\n",
    "                         for i in data[['candidate', 'mention']].itertuples()]\n",
    "    data['entityPrior'] = data['candidate'].map(get_entity_prior)\n",
    "    data['maxPriorProb'] = data['candidate'].map(max_prob)\n",
    "    \n",
    "    # String similarity features\n",
    "    ment_normalised = data['mention'].map(lambda x: x.lower())\n",
    "    cand_normalised = data['candidate'].map(lambda x: x.lower().replace('_', ' '))\n",
    "    ment_cand = list(zip(ment_normalised, cand_normalised))\n",
    "    data['editDist'] = [get_edit_dist(m, c) for m, c in ment_cand]\n",
    "    data['mentionIsCand'] = [m == c for m, c in ment_cand]\n",
    "    data['mentionInCand'] = [m in c for m, c in ment_cand]\n",
    "    data['isStartorEnd'] = [c.startswith(m) or c.endswith(m) for m, c in ment_cand]\n",
    "\n",
    "    # Context based features\n",
    "    # Context similarity \n",
    "    context_emb = gbrt.encode_sentence(get_document(i))\n",
    "    data['contextSim'] = data['candidate'].map(\n",
    "        lambda x: cos_sim(gbrt.encode_entity(x), context_emb))\n",
    "    # Coherence score\n",
    "    unamb_entities = data[data['priorProb'] >= 0.95]['candidate'].unique()\n",
    "    context_ent_emb = gbrt.encode_context_entities(unamb_entities)\n",
    "    data['coherence'] = data['candidate'].map(\n",
    "        lambda x: cos_sim(gbrt.encode_entity(x), context_ent_emb))\n",
    "    # Add rank\n",
    "    # data = rank_values(data)\n",
    "\n",
    "    # Add ground truth\n",
    "    data['y'] = (data['candidate'] == data['tag']).map(int)\n",
    "    dfs.append(data)\n",
    "\n",
    "X = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  add rank\n",
    "dfs = []\n",
    "while X.shape[0] != 0:\n",
    "    n = X.iloc[0]['numCands']\n",
    "    temp = X.head(n).copy()\n",
    "    temp['score'] = temp.contextSim\t+ temp.coherence\n",
    "    temp = temp.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "    temp['rank'] = temp.index + 1\n",
    "    X = X.iloc[n:]\n",
    "    dfs.append(temp)\n",
    "\n",
    "print(len(dfs))\n",
    "X = pd.concat(dfs).reset_index(drop=True)\n",
    "X.to_csv('./data/GBRT/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GBRT Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./data/GBRT/train.csv')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.drop(columns=['mention', 'candidate', 'tag', 'y'])\n",
    "y_train = X['y'].to_numpy()\n",
    "\n",
    "BASE = ['priorProb', 'entityPrior', 'maxPriorProb', 'numCands']\n",
    "STRING_SIM = BASE + ['editDist', 'mentionIsCand', 'mentionInCand', 'isStartorEnd']\n",
    "CONTEXT = STRING_SIM + ['contextSim']\n",
    "ALL = CONTEXT + ['coherence', 'rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "model.fit(X_train[BASE].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "model.fit(X_train[STRING_SIM].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/string_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "model.fit(X_train[CONTEXT].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/context.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "model.fit(X_train[ALL[:-1]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/coherence_no_rank.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "model.fit(X_train.to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/coherence.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3c83394ae731fac5cbf34b5abe7ebcd59fb96b846f104eed2689aeb9dd8ae81"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NED-using-KG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
