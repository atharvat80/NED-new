{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.gbrt import (GBRT, get_edit_dist, get_entity_prior,\n",
    "                      get_max_prior_prob, get_prior_prob)\n",
    "from src.utils import cos_sim, get_document, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = os.path.join(\"wiki2vec_w10_100d.pkl\", 'embeddings')\n",
    "\n",
    "features = ['priorProb', 'entityPrior', 'maxPriorProb', 'numCands',\n",
    "            'editDist', 'mentionIsCand', 'mentionInCand', 'isStartorEnd',\n",
    "            'contextSim', 'coherence', 'rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(model):\n",
    "    dfs = []\n",
    "    for i in tqdm(range(1, 1163)):\n",
    "        data = pd.read_csv(f'./data/aida/candidates/{i}.csv')\n",
    "        mentions = data['mention'].unique()\n",
    "        candidates = data['candidate'].unique()\n",
    "        max_prob = get_max_prior_prob(mentions, candidates)\n",
    "        \n",
    "        # Base features\n",
    "        data['priorProb'] = [get_prior_prob(i[1], i[2])\n",
    "                            for i in data[['candidate', 'mention']].itertuples()]\n",
    "        data['entityPrior'] = data['candidate'].map(get_entity_prior)\n",
    "        data['maxPriorProb'] = data['candidate'].map(max_prob)\n",
    "        \n",
    "        # String similarity features\n",
    "        ment_normalised = data['mention'].map(lambda x: x.lower())\n",
    "        cand_normalised = data['candidate'].map(lambda x: x.lower().replace('_', ' '))\n",
    "        ment_cand = list(zip(ment_normalised, cand_normalised))\n",
    "        data['editDist'] = [get_edit_dist(m, c) for m, c in ment_cand]\n",
    "        data['mentionIsCand'] = [m == c for m, c in ment_cand]\n",
    "        data['mentionInCand'] = [m in c for m, c in ment_cand]\n",
    "        data['isStartorEnd'] = [c.startswith(m) or c.endswith(m) for m, c in ment_cand]\n",
    "\n",
    "        # Context based features\n",
    "        # Context similarity \n",
    "        context_emb = model.encode_sentence(get_document(i))\n",
    "        data['contextSim'] = data['candidate'].map(\n",
    "            lambda x: cos_sim(model.encode_entity(x), context_emb))\n",
    "        # Coherence score\n",
    "        unamb_entities = data[data['priorProb'] >= 0.95]['candidate'].unique()\n",
    "        context_ent_emb = model.encode_context_entities(unamb_entities)\n",
    "        data['coherence'] = data['candidate'].map(\n",
    "            lambda x: cos_sim(model.encode_entity(x), context_ent_emb))\n",
    "\n",
    "        # Add ground truth\n",
    "        data['y'] = (data['candidate'] == data['tag']).map(int)\n",
    "        dfs.append(data)\n",
    "\n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    #  add rank\n",
    "    dfs = []\n",
    "    while X.shape[0] != 0:\n",
    "        n = X.iloc[0]['numCands']\n",
    "        temp = X.head(n).copy()\n",
    "        temp['score'] = temp.contextSim\t+ temp.coherence\n",
    "        temp = temp.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        temp['rank'] = temp.index + 1\n",
    "        X = X.iloc[n:]\n",
    "        dfs.append(temp)\n",
    "        \n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "    return X.drop(columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs  = [f\"wiki2vec_w10_{i}d.pkl\" for i in [100, 300]] \n",
    "embs += ['word2vec-google-news-300', 'glove-wiki-gigaword-300']\n",
    "entity_desc_dict = load_json(os.path.join(os.getcwd(), 'data', 'aida', 'entities.json'))\n",
    "for emb in embs:\n",
    "    model = GBRT(os.path.join(EMB_PATH, emb), cased = 'word2vec' in emb)\n",
    "    model.entity_desc_dict = entity_desc_dict\n",
    "    train_df = generate_train_data(model)\n",
    "    train_df.to_csv(f\"./data/GBRT/{emb}_train.csv\", index=False)\n",
    "    model = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GBRT (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"./data/GBRT/wiki2vec_w10_300d.pkl_train.csv\")\n",
    "y_train = X['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[features[:4]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[features[:8]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/string_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[features[:9]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/context.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[features].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/coherence.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719          110.48m\n",
      "         2           0.0698          109.41m\n",
      "         3           0.0677           98.74m\n",
      "         4           0.0657           90.78m\n",
      "         5           0.0638           87.18m\n",
      "         6           0.0619           85.11m\n",
      "         7           0.0602           83.02m\n",
      "         8           0.0584           81.49m\n",
      "         9           0.0568           80.08m\n",
      "        10           0.0552           79.73m\n",
      "        20           0.0424           72.88m\n",
      "        30           0.0335           71.25m\n",
      "        40           0.0273           70.67m\n",
      "        50           0.0231           70.20m\n",
      "        60           0.0201           69.57m\n",
      "        70           0.0180           69.21m\n",
      "        80           0.0165           69.23m\n",
      "        90           0.0154           69.11m\n",
      "       100           0.0146           69.30m\n",
      "       200           0.0120           67.76m\n",
      "       300           0.0113           65.80m\n",
      "       400           0.0109           64.90m\n",
      "       500           0.0105           64.25m\n",
      "       600           0.0103           65.47m\n",
      "       700           0.0101           67.62m\n",
      "       800           0.0099           66.67m\n",
      "       900           0.0097           65.72m\n",
      "      1000           0.0095           64.71m\n",
      "      2000           0.0084           55.53m\n",
      "      3000           0.0075           47.86m\n",
      "      4000           0.0069           40.27m\n",
      "      5000           0.0063           32.76m\n",
      "      6000           0.0059           25.78m\n",
      "      7000           0.0055           19.26m\n",
      "      8000           0.0051           12.84m\n",
      "      9000           0.0048            6.42m\n",
      "     10000           0.0046            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719           84.22m\n",
      "         2           0.0698           85.93m\n",
      "         3           0.0677           83.63m\n",
      "         4           0.0657           79.87m\n",
      "         5           0.0638           77.38m\n",
      "         6           0.0620           76.45m\n",
      "         7           0.0602           75.14m\n",
      "         8           0.0586           74.64m\n",
      "         9           0.0570           74.19m\n",
      "        10           0.0555           73.67m\n",
      "        20           0.0430           70.91m\n",
      "        30           0.0342           70.10m\n",
      "        40           0.0282           69.58m\n",
      "        50           0.0241           69.21m\n",
      "        60           0.0212           68.58m\n",
      "        70           0.0191           68.25m\n",
      "        80           0.0176           68.02m\n",
      "        90           0.0166           67.98m\n",
      "       100           0.0158           67.80m\n",
      "       200           0.0130           65.88m\n",
      "       300           0.0122           64.66m\n",
      "       400           0.0118           63.98m\n",
      "       500           0.0114           62.70m\n",
      "       600           0.0112           61.68m\n",
      "       700           0.0109           60.77m\n",
      "       800           0.0107           59.87m\n",
      "       900           0.0105           59.14m\n",
      "      1000           0.0103           58.36m\n",
      "      2000           0.0088           51.68m\n",
      "      3000           0.0080           44.99m\n",
      "      4000           0.0074           38.37m\n",
      "      5000           0.0068           31.97m\n",
      "      6000           0.0063           25.38m\n",
      "      7000           0.0059           18.89m\n",
      "      8000           0.0055           12.51m\n",
      "      9000           0.0052            6.22m\n",
      "     10000           0.0049            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           97.76m\n",
      "         2           0.0698           97.16m\n",
      "         3           0.0678           93.31m\n",
      "         4           0.0659           88.97m\n",
      "         5           0.0640           88.30m\n",
      "         6           0.0622           88.37m\n",
      "         7           0.0604           88.46m\n",
      "         8           0.0587           88.21m\n",
      "         9           0.0571           88.08m\n",
      "        10           0.0556           88.09m\n",
      "        20           0.0431           76.53m\n",
      "        30           0.0346           72.90m\n",
      "        40           0.0288           70.93m\n",
      "        50           0.0248           69.70m\n",
      "        60           0.0219           68.46m\n",
      "        70           0.0200           67.94m\n",
      "        80           0.0185           67.53m\n",
      "        90           0.0175           67.12m\n",
      "       100           0.0168           66.88m\n",
      "       200           0.0142           65.23m\n",
      "       300           0.0135           64.25m\n",
      "       400           0.0130           63.60m\n",
      "       500           0.0126           62.80m\n",
      "       600           0.0122           62.15m\n",
      "       700           0.0119           61.47m\n",
      "       800           0.0117           60.84m\n",
      "       900           0.0115           60.67m\n",
      "      1000           0.0113           59.86m\n",
      "      2000           0.0098           52.60m\n",
      "      3000           0.0088           46.06m\n",
      "      4000           0.0080           39.53m\n",
      "      5000           0.0073           32.64m\n",
      "      6000           0.0068           25.65m\n",
      "      7000           0.0063           18.98m\n",
      "      8000           0.0059           12.51m\n",
      "      9000           0.0056            6.20m\n",
      "     10000           0.0053            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           61.08m\n",
      "         2           0.0699           65.23m\n",
      "         3           0.0680           67.00m\n",
      "         4           0.0661           65.48m\n",
      "         5           0.0642           65.34m\n",
      "         6           0.0625           65.21m\n",
      "         7           0.0608           64.60m\n",
      "         8           0.0592           64.52m\n",
      "         9           0.0576           63.83m\n",
      "        10           0.0561           63.53m\n",
      "        20           0.0438           61.87m\n",
      "        30           0.0354           61.27m\n",
      "        40           0.0295           60.42m\n",
      "        50           0.0255           60.21m\n",
      "        60           0.0228           60.12m\n",
      "        70           0.0208           59.73m\n",
      "        80           0.0194           59.57m\n",
      "        90           0.0184           59.46m\n",
      "       100           0.0177           59.33m\n",
      "       200           0.0152           58.46m\n",
      "       300           0.0144           57.97m\n",
      "       400           0.0139           57.46m\n",
      "       500           0.0135           56.71m\n",
      "       600           0.0132           55.91m\n",
      "       700           0.0129           55.22m\n",
      "       800           0.0126           54.99m\n",
      "       900           0.0124           54.20m\n",
      "      1000           0.0121           53.48m\n",
      "      2000           0.0104           46.97m\n",
      "      3000           0.0093           40.77m\n",
      "      4000           0.0085           34.78m\n",
      "      5000           0.0078           28.91m\n",
      "      6000           0.0072           23.07m\n",
      "      7000           0.0067           17.27m\n",
      "      8000           0.0063           11.49m\n",
      "      9000           0.0059            5.75m\n",
      "     10000           0.0056            0.00s\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)\n",
    "\n",
    "embs  = [f\"wiki2vec_w10_{i}d.pkl\" for i in [100, 300]] \n",
    "embs += ['word2vec-google-news-300', 'glove-wiki-gigaword-300']\n",
    "\n",
    "for emb in embs:\n",
    "    X = pd.read_csv(f\"./data/GBRT/{emb}_train.csv\")\n",
    "    model.fit(X[features].to_numpy(), X['y'].to_numpy())\n",
    "    save_model(model, f\"./data/GBRT/{emb}_trained.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3c83394ae731fac5cbf34b5abe7ebcd59fb96b846f104eed2689aeb9dd8ae81"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NED-using-KG': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
