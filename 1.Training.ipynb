{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.gbrt import GBRT\n",
    "from src.transformer import GBRT_TRF\n",
    "from src.gbrt import get_edit_dist, get_entity_prior, get_max_prior_prob, get_prior_prob\n",
    "from src.utils import cosine_similarity, get_document, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = os.path.join(os.getcwd(), 'embeddings')\n",
    "\n",
    "features = ['priorProb', 'entityPrior', 'maxPriorProb', 'numCands',\n",
    "            'editDist', 'mentionIsCand', 'mentionInCand', 'isStartorEnd',\n",
    "            'contextSim', 'coherence', 'rank']\n",
    "\n",
    "embs  = [\"wiki2vec_w10_100d.pkl\", \"wiki2vec_w10_300d.pkl\", \n",
    "         \"word2vec-google-news-300\", \"glove-wiki-gigaword-300\",\n",
    "         \"fasttext-wiki-news-subwords-300\", \"en.wiki.bpe.vs200000.d300.w2v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(model):\n",
    "    dfs = []\n",
    "    for i in tqdm(range(1, 1163)):\n",
    "        data = pd.read_csv(f'./data/aida/candidates/{i}.csv')\n",
    "        mentions = data['mention'].unique()\n",
    "        candidates = data['candidate'].unique()\n",
    "        max_prob = get_max_prior_prob(mentions, candidates)\n",
    "        \n",
    "        # Base features\n",
    "        data['priorProb'] = [get_prior_prob(i[1], i[2])\n",
    "                            for i in data[['candidate', 'mention']].itertuples()]\n",
    "        data['entityPrior'] = data['candidate'].map(get_entity_prior)\n",
    "        data['maxPriorProb'] = data['candidate'].map(max_prob)\n",
    "        \n",
    "        # String similarity features\n",
    "        ment_normalised = data['mention'].map(lambda x: x.lower())\n",
    "        cand_normalised = data['candidate'].map(lambda x: x.lower().replace('_', ' '))\n",
    "        ment_cand = list(zip(ment_normalised, cand_normalised))\n",
    "        data['editDist'] = [get_edit_dist(m, c) for m, c in ment_cand]\n",
    "        data['mentionIsCand'] = [int(m == c) for m, c in ment_cand]\n",
    "        data['mentionInCand'] = [int(m in c) for m, c in ment_cand]\n",
    "        data['isStartorEnd'] = [int(c.startswith(m) or c.endswith(m)) for m, c in ment_cand]\n",
    "\n",
    "        # Context based features\n",
    "        # Context similarity \n",
    "        context_emb = model.encode_sentence(get_document(i))\n",
    "        data['contextSim'] = data['candidate'].map(lambda x: cosine_similarity(model.encode_entity(x), context_emb))\n",
    "        # Coherence score\n",
    "        unamb_entities = data[data['priorProb'] >= 0.95]['candidate'].unique()\n",
    "        context_ent_emb = model.encode_context_entities(unamb_entities)\n",
    "        data['coherence'] = data['candidate'].map(lambda x: cosine_similarity(model.encode_entity(x), context_ent_emb))\n",
    "\n",
    "        # Add ground truth\n",
    "        data['y'] = (data['candidate'] == data['tag']).map(int)\n",
    "        dfs.append(data)\n",
    "\n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    #  add rank\n",
    "    dfs = []\n",
    "    while X.shape[0] != 0:\n",
    "        n = X.iloc[0]['numCands']\n",
    "        temp = X.head(n).copy()\n",
    "        temp['score'] = temp.contextSim\t+ temp.coherence\n",
    "        temp = temp.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        temp['rank'] = temp.index + 1\n",
    "        X = X.iloc[n:]\n",
    "        dfs.append(temp)\n",
    "        \n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "    return X.drop(columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_desc_dict = load_json(os.path.join(os.getcwd(), 'data', 'aida', 'entities.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb in embs[3:]:\n",
    "    model = GBRT(os.path.join(EMB_PATH, emb), cased = 'word2vec' in emb)\n",
    "    model.entity_desc_dict = entity_desc_dict\n",
    "    train_df = generate_train_data(model)\n",
    "    train_df.to_csv(f\"./data/GBRT/{emb}_train.csv\", index=False)\n",
    "    model = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eccd5679d224017b082d2e1b7f35f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GBRT_TRF()\n",
    "model.entity_desc_dict = entity_desc_dict\n",
    "train_df = generate_train_data(model)\n",
    "train_df.to_csv(f\"./data/GBRT/TRF_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the GBRT (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                  max_depth=4, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(f\"./data/GBRT/{embs[0]}_train.csv\")\n",
    "y_train = X['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0722           29.33m\n",
      "         2           0.0703           30.08m\n",
      "         3           0.0685           29.21m\n",
      "         4           0.0668           31.40m\n",
      "         5           0.0651           32.69m\n",
      "         6           0.0635           34.41m\n",
      "         7           0.0619           36.15m\n",
      "         8           0.0604           34.92m\n",
      "         9           0.0590           35.16m\n",
      "        10           0.0577           35.27m\n",
      "        20           0.0466           36.53m\n",
      "        30           0.0391           35.23m\n",
      "        40           0.0341           34.07m\n",
      "        50           0.0306           32.36m\n",
      "        60           0.0283           31.48m\n",
      "        70           0.0267           31.18m\n",
      "        80           0.0255           31.00m\n",
      "        90           0.0247           30.73m\n",
      "       100           0.0241           30.56m\n",
      "       200           0.0219           30.97m\n",
      "       300           0.0212           31.31m\n",
      "       400           0.0206           31.19m\n",
      "       500           0.0201           29.55m\n",
      "       600           0.0197           28.40m\n",
      "       700           0.0193           27.44m\n",
      "       800           0.0189           26.66m\n",
      "       900           0.0186           26.01m\n",
      "      1000           0.0182           25.40m\n",
      "      2000           0.0156           21.62m\n",
      "      3000           0.0139           18.60m\n",
      "      4000           0.0126           15.88m\n",
      "      5000           0.0116           13.22m\n",
      "      6000           0.0108           10.55m\n",
      "      7000           0.0101            7.91m\n",
      "      8000           0.0096            5.28m\n",
      "      9000           0.0091            2.64m\n",
      "     10000           0.0087            0.00s\n"
     ]
    }
   ],
   "source": [
    "model.fit(X[features[:4]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0721           42.77m\n",
      "         2           0.0702           44.71m\n",
      "         3           0.0683           43.13m\n",
      "         4           0.0665           46.09m\n",
      "         5           0.0648           50.56m\n",
      "         6           0.0632           51.01m\n",
      "         7           0.0616           49.95m\n",
      "         8           0.0600           50.11m\n",
      "         9           0.0586           49.33m\n",
      "        10           0.0572           49.51m\n",
      "        20           0.0457           42.75m\n",
      "        30           0.0380           41.20m\n",
      "        40           0.0328           40.10m\n",
      "        50           0.0292           39.42m\n",
      "        60           0.0268           38.68m\n",
      "        70           0.0251           38.43m\n",
      "        80           0.0239           38.09m\n",
      "        90           0.0230           37.95m\n",
      "       100           0.0224           37.67m\n",
      "       200           0.0200           36.14m\n",
      "       300           0.0192           35.35m\n",
      "       400           0.0186           34.84m\n",
      "       500           0.0181           34.52m\n",
      "       600           0.0176           34.12m\n",
      "       700           0.0171           33.86m\n",
      "       800           0.0167           33.54m\n",
      "       900           0.0164           33.17m\n",
      "      1000           0.0160           32.83m\n",
      "      2000           0.0138           28.74m\n",
      "      3000           0.0125           25.07m\n",
      "      4000           0.0114           21.30m\n",
      "      5000           0.0106           17.68m\n",
      "      6000           0.0099           14.04m\n",
      "      7000           0.0093           10.51m\n",
      "      8000           0.0088            6.97m\n",
      "      9000           0.0084            3.47m\n",
      "     10000           0.0081            0.00s\n"
     ]
    }
   ],
   "source": [
    "model.fit(X[features[:8]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/string_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           60.83m\n",
      "         2           0.0700           60.82m\n",
      "         3           0.0681           61.57m\n",
      "         4           0.0662           67.20m\n",
      "         5           0.0644           65.52m\n",
      "         6           0.0626           63.92m\n",
      "         7           0.0610           62.74m\n",
      "         8           0.0594           61.55m\n",
      "         9           0.0578           60.66m\n",
      "        10           0.0564           60.03m\n",
      "        20           0.0443           55.39m\n",
      "        30           0.0361           54.13m\n",
      "        40           0.0305           53.65m\n",
      "        50           0.0265           53.31m\n",
      "        60           0.0237           52.88m\n",
      "        70           0.0218           52.55m\n",
      "        80           0.0204           52.31m\n",
      "        90           0.0194           51.99m\n",
      "       100           0.0185           51.63m\n",
      "       200           0.0158           49.69m\n",
      "       300           0.0149           48.68m\n",
      "       400           0.0142           48.18m\n",
      "       500           0.0137           47.56m\n",
      "       600           0.0133           46.99m\n",
      "       700           0.0130           46.54m\n",
      "       800           0.0128           47.44m\n",
      "       900           0.0126           46.37m\n",
      "      1000           0.0123           45.44m\n",
      "      2000           0.0105           39.86m\n",
      "      3000           0.0095           34.46m\n",
      "      4000           0.0086           29.33m\n",
      "      5000           0.0080           24.36m\n",
      "      6000           0.0075           19.67m\n",
      "      7000           0.0070           14.86m\n",
      "      8000           0.0066            9.96m\n",
      "      9000           0.0062            5.00m\n",
      "     10000           0.0059            0.00s\n"
     ]
    }
   ],
   "source": [
    "model.fit(X[features[:9]].to_numpy(), y_train)\n",
    "save_model(model, './data/GBRT/context.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719           90.33m\n",
      "         2           0.0698           93.03m\n",
      "         3           0.0677           90.84m\n",
      "         4           0.0657           91.61m\n",
      "         5           0.0638           91.06m\n",
      "         6           0.0619           90.36m\n",
      "         7           0.0602           89.10m\n",
      "         8           0.0584           88.48m\n",
      "         9           0.0568           87.63m\n",
      "        10           0.0552           86.96m\n",
      "        20           0.0424           82.55m\n",
      "        30           0.0335           81.26m\n",
      "        40           0.0273           80.40m\n",
      "        50           0.0231           79.97m\n",
      "        60           0.0201           79.43m\n",
      "        70           0.0180           78.88m\n",
      "        80           0.0165           78.60m\n",
      "        90           0.0154           78.29m\n",
      "       100           0.0146           78.06m\n",
      "       200           0.0120           76.00m\n",
      "       300           0.0113           75.61m\n",
      "       400           0.0109           75.00m\n",
      "       500           0.0105           74.26m\n",
      "       600           0.0103           73.12m\n",
      "       700           0.0101           72.20m\n",
      "       800           0.0099           71.00m\n",
      "       900           0.0097           70.00m\n",
      "      1000           0.0095           68.95m\n",
      "      2000           0.0084           62.54m\n",
      "      3000           0.0075           61.95m\n",
      "      4000           0.0069           63.19m\n",
      "      5000           0.0063           61.76m\n",
      "      6000           0.0059           50.42m\n",
      "      7000           0.0055           37.70m\n",
      "      8000           0.0051           24.72m\n",
      "      9000           0.0048           12.31m\n",
      "     10000           0.0046            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0719          149.07m\n",
      "         2           0.0698          135.75m\n",
      "         3           0.0677          130.52m\n",
      "         4           0.0657          129.69m\n",
      "         5           0.0638          128.79m\n",
      "         6           0.0620          126.66m\n",
      "         7           0.0602          124.25m\n",
      "         8           0.0586          123.82m\n",
      "         9           0.0570          122.81m\n",
      "        10           0.0555          121.63m\n",
      "        20           0.0430          114.97m\n",
      "        30           0.0342          112.52m\n",
      "        40           0.0282          110.38m\n",
      "        50           0.0241          110.87m\n",
      "        60           0.0212          110.57m\n",
      "        70           0.0191          110.83m\n",
      "        80           0.0176          110.81m\n",
      "        90           0.0166          110.68m\n",
      "       100           0.0158          110.52m\n",
      "       200           0.0130          111.37m\n",
      "       300           0.0122          107.67m\n",
      "       400           0.0118          101.49m\n",
      "       500           0.0114           93.62m\n",
      "       600           0.0112           88.11m\n",
      "       700           0.0109           84.01m\n",
      "       800           0.0107           80.70m\n",
      "       900           0.0105           78.57m\n",
      "      1000           0.0103           76.50m\n",
      "      2000           0.0088           74.65m\n",
      "      3000           0.0080           68.56m\n",
      "      4000           0.0074           61.48m\n",
      "      5000           0.0068           49.18m\n",
      "      6000           0.0063           37.92m\n",
      "      7000           0.0059           27.44m\n",
      "      8000           0.0055           17.79m\n",
      "      9000           0.0052            8.77m\n",
      "     10000           0.0049            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720          149.36m\n",
      "         2           0.0698          126.25m\n",
      "         3           0.0678          126.70m\n",
      "         4           0.0658          124.31m\n",
      "         5           0.0640          123.83m\n",
      "         6           0.0621          118.00m\n",
      "         7           0.0604          113.01m\n",
      "         8           0.0587          110.51m\n",
      "         9           0.0571          107.88m\n",
      "        10           0.0556          105.40m\n",
      "        20           0.0430           93.76m\n",
      "        30           0.0345           91.23m\n",
      "        40           0.0288           94.80m\n",
      "        50           0.0247           92.94m\n",
      "        60           0.0219           92.80m\n",
      "        70           0.0199           91.43m\n",
      "        80           0.0185           90.01m\n",
      "        90           0.0175           89.21m\n",
      "       100           0.0167           90.53m\n",
      "       200           0.0142          105.13m\n",
      "       300           0.0135           98.21m\n",
      "       400           0.0129           93.00m\n",
      "       500           0.0125           90.81m\n",
      "       600           0.0121           93.45m\n",
      "       700           0.0118           92.84m\n",
      "       800           0.0116           91.89m\n",
      "       900           0.0114           90.35m\n",
      "      1000           0.0112           88.77m\n",
      "      2000           0.0097           79.30m\n",
      "      3000           0.0088           67.50m\n",
      "      4000           0.0080           57.38m\n",
      "      5000           0.0074           48.36m\n",
      "      6000           0.0068           37.45m\n",
      "      7000           0.0064           27.83m\n",
      "      8000           0.0060           18.51m\n",
      "      9000           0.0057            9.23m\n",
      "     10000           0.0054            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           90.85m\n",
      "         2           0.0699           86.17m\n",
      "         3           0.0680           86.14m\n",
      "         4           0.0661           85.57m\n",
      "         5           0.0642           86.68m\n",
      "         6           0.0625           87.19m\n",
      "         7           0.0608           87.12m\n",
      "         8           0.0591           87.06m\n",
      "         9           0.0576           86.58m\n",
      "        10           0.0561           86.27m\n",
      "        20           0.0438           88.88m\n",
      "        30           0.0353           85.48m\n",
      "        40           0.0295           84.15m\n",
      "        50           0.0255           83.08m\n",
      "        60           0.0227           83.20m\n",
      "        70           0.0208           82.44m\n",
      "        80           0.0193           81.74m\n",
      "        90           0.0183           81.33m\n",
      "       100           0.0176           81.21m\n",
      "       200           0.0151           78.38m\n",
      "       300           0.0143           78.83m\n",
      "       400           0.0138           77.80m\n",
      "       500           0.0135           77.36m\n",
      "       600           0.0132           77.10m\n",
      "       700           0.0129           76.65m\n",
      "       800           0.0127           76.37m\n",
      "       900           0.0124           76.35m\n",
      "      1000           0.0122           75.53m\n",
      "      2000           0.0105           68.10m\n",
      "      3000           0.0093           59.80m\n",
      "      4000           0.0085           51.64m\n",
      "      5000           0.0078           43.69m\n",
      "      6000           0.0072           35.04m\n",
      "      7000           0.0067           26.56m\n",
      "      8000           0.0063           17.87m\n",
      "      9000           0.0059            8.85m\n",
      "     10000           0.0056            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0721          182.81m\n",
      "         2           0.0701          177.96m\n",
      "         3           0.0682          162.87m\n",
      "         4           0.0664          154.49m\n",
      "         5           0.0646          143.19m\n",
      "         6           0.0629          135.48m\n",
      "         7           0.0613          129.12m\n",
      "         8           0.0597          123.89m\n",
      "         9           0.0582          119.70m\n",
      "        10           0.0567          116.25m\n",
      "        20           0.0449          100.63m\n",
      "        30           0.0368           94.98m\n",
      "        40           0.0313           92.11m\n",
      "        50           0.0275           90.24m\n",
      "        60           0.0247           88.72m\n",
      "        70           0.0228           87.36m\n",
      "        80           0.0214           87.09m\n",
      "        90           0.0203           86.64m\n",
      "       100           0.0195           86.32m\n",
      "       200           0.0168           82.15m\n",
      "       300           0.0161           80.97m\n",
      "       400           0.0156           80.85m\n",
      "       500           0.0151           81.79m\n",
      "       600           0.0148           86.03m\n",
      "       700           0.0145           94.54m\n",
      "       800           0.0142           99.09m\n",
      "       900           0.0139          102.02m\n",
      "      1000           0.0136          103.00m\n",
      "      2000           0.0115           93.99m\n",
      "      3000           0.0103           79.79m\n",
      "      4000           0.0094           66.30m\n",
      "      5000           0.0086           52.72m\n",
      "      6000           0.0080           40.74m\n",
      "      7000           0.0074           29.66m\n",
      "      8000           0.0070           19.35m\n",
      "      9000           0.0066            9.50m\n",
      "     10000           0.0062            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0721           94.66m\n",
      "         2           0.0700           96.06m\n",
      "         3           0.0681           94.55m\n",
      "         4           0.0662           92.80m\n",
      "         5           0.0644           91.60m\n",
      "         6           0.0627           92.37m\n",
      "         7           0.0611           91.65m\n",
      "         8           0.0595           91.20m\n",
      "         9           0.0579           92.43m\n",
      "        10           0.0565           92.14m\n",
      "        20           0.0444           86.34m\n",
      "        30           0.0362           84.91m\n",
      "        40           0.0306           83.60m\n",
      "        50           0.0267           83.10m\n",
      "        60           0.0240           82.44m\n",
      "        70           0.0219           82.10m\n",
      "        80           0.0204           81.92m\n",
      "        90           0.0194           81.46m\n",
      "       100           0.0187           81.18m\n",
      "       200           0.0160           79.27m\n",
      "       300           0.0151           78.03m\n",
      "       400           0.0146           78.73m\n",
      "       500           0.0141           77.35m\n",
      "       600           0.0137           76.27m\n",
      "       700           0.0134           75.18m\n",
      "       800           0.0131           74.18m\n",
      "       900           0.0129           73.29m\n",
      "      1000           0.0127           72.55m\n",
      "      2000           0.0110           63.98m\n",
      "      3000           0.0098           55.93m\n",
      "      4000           0.0089           47.85m\n",
      "      5000           0.0082           39.85m\n",
      "      6000           0.0076           31.86m\n",
      "      7000           0.0070           23.89m\n",
      "      8000           0.0066           15.92m\n",
      "      9000           0.0063            7.96m\n",
      "     10000           0.0059            0.00s\n"
     ]
    }
   ],
   "source": [
    "for emb in embs:\n",
    "    X = pd.read_csv(f\"./data/GBRT/{emb}_train.csv\")\n",
    "    model.fit(X[features].to_numpy(), X['y'].to_numpy())\n",
    "    save_model(model, f\"./data/GBRT/{emb}_trained.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720           68.00m\n",
      "         2           0.0699           64.11m\n",
      "         3           0.0679           63.64m\n",
      "         4           0.0660           62.79m\n",
      "         5           0.0642           62.77m\n",
      "         6           0.0624           62.56m\n",
      "         7           0.0607           62.01m\n",
      "         8           0.0591           62.11m\n",
      "         9           0.0575           61.89m\n",
      "        10           0.0561           61.70m\n",
      "        20           0.0439           60.86m\n",
      "        30           0.0357           60.63m\n",
      "        40           0.0301           60.35m\n",
      "        50           0.0262           60.30m\n",
      "        60           0.0235           60.17m\n",
      "        70           0.0215           60.06m\n",
      "        80           0.0201           60.00m\n",
      "        90           0.0191           59.96m\n",
      "       100           0.0184           59.89m\n",
      "       200           0.0157           61.72m\n",
      "       300           0.0149           61.41m\n",
      "       400           0.0143           60.91m\n",
      "       500           0.0138           60.20m\n",
      "       600           0.0135           59.70m\n",
      "       700           0.0131           59.05m\n",
      "       800           0.0129           58.47m\n",
      "       900           0.0126           59.20m\n",
      "      1000           0.0124           59.39m\n",
      "      2000           0.0108           53.32m\n",
      "      3000           0.0097           46.38m\n",
      "      4000           0.0088           39.72m\n",
      "      5000           0.0081           33.05m\n",
      "      6000           0.0076           26.45m\n",
      "      7000           0.0070           19.85m\n",
      "      8000           0.0066           13.23m\n",
      "      9000           0.0062            6.61m\n",
      "     10000           0.0059            0.00s\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(f\"./data/GBRT/TRF_train.csv\")\n",
    "model.fit(X[features].to_numpy(), X['y'].to_numpy())\n",
    "save_model(model, f\"./data/GBRT/TRF_trained.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3c83394ae731fac5cbf34b5abe7ebcd59fb96b846f104eed2689aeb9dd8ae81"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
