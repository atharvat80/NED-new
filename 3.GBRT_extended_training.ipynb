{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GBRT data files...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.gbrt import *\n",
    "from src.utils import cos_sim, get_document, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH = \"C:\\\\Personal Files\\\\NED-using-KG\\\\embeddings\\\\\"\n",
    "EMB_TYPE = ['word2vec-google-news-300', 'glove-wiki-gigaword-300', \n",
    "            'fasttext-wiki-news-subwords-300', 'en.wiki.bpe.vs200000.d300.w2v']\n",
    "WIKI2VEC = EMB_PATH + 'wiki2vec_w10_100d.pkl'\n",
    "\n",
    "entity_desc = load_json(\"C:\\\\Personal Files\\\\NED-using-KG\\\\data\\\\aida\\\\entities.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def generate_test_data(emb, is_wiki2vec=False):\n",
    "    dfs = []\n",
    "    gbrt = GBRT(emb, is_wiki2vec=is_wiki2vec)\n",
    "    gbrt.cached_entity_desc = entity_desc\n",
    "    for i in tqdm(range(1, 1163)):\n",
    "        data = pd.read_csv(f'./data/aida/candidates/{i}.csv')\n",
    "        mentions = data['mention'].unique()\n",
    "        candidates = data['candidate'].unique()\n",
    "        max_prob = get_max_prior_prob(mentions, candidates)\n",
    "        \n",
    "        # Base features\n",
    "        data['priorProb'] = [get_prior_prob(i[1], i[2])\n",
    "                            for i in data[['candidate', 'mention']].itertuples()]\n",
    "        data['entityPrior'] = data['candidate'].map(get_entity_prior)\n",
    "        data['maxPriorProb'] = data['candidate'].map(max_prob)\n",
    "        \n",
    "        # String similarity features\n",
    "        ment_normalised = data['mention'].map(lambda x: x.lower())\n",
    "        cand_normalised = data['candidate'].map(lambda x: x.lower().replace('_', ' '))\n",
    "        ment_cand = list(zip(ment_normalised, cand_normalised))\n",
    "        data['editDist'] = [get_edit_dist(m, c) for m, c in ment_cand]\n",
    "        data['mentionIsCand'] = [m == c for m, c in ment_cand]\n",
    "        data['mentionInCand'] = [m in c for m, c in ment_cand]\n",
    "        data['isStartorEnd'] = [c.startswith(m) or c.endswith(m) for m, c in ment_cand]\n",
    "\n",
    "        # Context based features\n",
    "        # Context similarity \n",
    "        context_emb = gbrt.encode_sentence(get_document(i))\n",
    "        data['contextSim'] = data['candidate'].map(\n",
    "            lambda x: cos_sim(gbrt.encode_entity(x), context_emb))\n",
    "        # Coherence score\n",
    "        unamb_entities = data[data['priorProb'] >= 0.95]['candidate'].unique()\n",
    "        context_ent_emb = gbrt.encode_context_entities(unamb_entities)\n",
    "        data['coherence'] = data['candidate'].map(\n",
    "            lambda x: cos_sim(gbrt.encode_entity(x), context_ent_emb))\n",
    "\n",
    "        # Add ground truth\n",
    "        data['y'] = (data['candidate'] == data['tag']).map(int)\n",
    "        dfs.append(data)\n",
    "\n",
    "    X = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    #  add rank\n",
    "    dfs = []\n",
    "    while X.shape[0] != 0:\n",
    "        n = X.iloc[0]['numCands']\n",
    "        temp = X.head(n).copy()\n",
    "        temp['score'] = temp.contextSim\t+ temp.coherence\n",
    "        temp = temp.sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "        temp['rank'] = temp.index + 1\n",
    "        X = X.iloc[n:]\n",
    "        dfs.append(temp)\n",
    "\n",
    "    print(len(dfs))\n",
    "    return pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb in EMB_TYPE:\n",
    "    X_train = generate_test_data(EMB_PATH + emb)\n",
    "    X_train.drop(columns=['mention', 'candidate', 'tag', 'score']).to_csv(f\"./data/GBRT/{emb}_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0720          115.17m\n",
      "         2           0.0699          116.91m\n",
      "         3           0.0679          120.21m\n",
      "         4           0.0660          122.16m\n",
      "         5           0.0642          121.70m\n",
      "         6           0.0624          119.10m\n",
      "         7           0.0607          114.08m\n",
      "         8           0.0591          110.52m\n",
      "         9           0.0575          107.35m\n",
      "        10           0.0560          104.56m\n",
      "        20           0.0437           91.31m\n",
      "        30           0.0352           86.10m\n",
      "        40           0.0294           83.57m\n",
      "        50           0.0254           82.55m\n",
      "        60           0.0226           82.47m\n",
      "        70           0.0206           81.40m\n",
      "        80           0.0193           80.56m\n",
      "        90           0.0182           81.68m\n",
      "       100           0.0175           80.90m\n",
      "       200           0.0150           75.63m\n",
      "       300           0.0143           73.10m\n",
      "       400           0.0139           71.50m\n",
      "       500           0.0134           70.33m\n",
      "       600           0.0132           69.27m\n",
      "       700           0.0129           68.55m\n",
      "       800           0.0127           67.63m\n",
      "       900           0.0124           65.67m\n",
      "      1000           0.0122           63.94m\n",
      "      2000           0.0105           55.82m\n",
      "      3000           0.0094           46.54m\n",
      "      4000           0.0086           39.15m\n",
      "      5000           0.0079           31.91m\n",
      "      6000           0.0073           25.19m\n",
      "      7000           0.0068           18.70m\n",
      "      8000           0.0064           12.36m\n",
      "      9000           0.0060            6.14m\n",
      "     10000           0.0056            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0721           60.75m\n",
      "         2           0.0701           60.96m\n",
      "         3           0.0682           66.63m\n",
      "         4           0.0664           69.50m\n",
      "         5           0.0646           69.87m\n",
      "         6           0.0629           68.61m\n",
      "         7           0.0613           67.14m\n",
      "         8           0.0597           66.35m\n",
      "         9           0.0582           66.04m\n",
      "        10           0.0567           65.52m\n",
      "        20           0.0449           63.38m\n",
      "        30           0.0368           62.01m\n",
      "        40           0.0314           61.75m\n",
      "        50           0.0275           61.42m\n",
      "        60           0.0248           60.96m\n",
      "        70           0.0229           60.83m\n",
      "        80           0.0215           60.53m\n",
      "        90           0.0203           60.43m\n",
      "       100           0.0196           60.41m\n",
      "       200           0.0169           59.21m\n",
      "       300           0.0162           58.32m\n",
      "       400           0.0156           57.56m\n",
      "       500           0.0151           56.79m\n",
      "       600           0.0148           56.16m\n",
      "       700           0.0144           55.97m\n",
      "       800           0.0141           55.28m\n",
      "       900           0.0139           54.64m\n",
      "      1000           0.0136           54.07m\n",
      "      2000           0.0117           47.98m\n",
      "      3000           0.0104           41.69m\n",
      "      4000           0.0095           35.50m\n",
      "      5000           0.0087           29.99m\n",
      "      6000           0.0081           24.29m\n",
      "      7000           0.0075           18.13m\n",
      "      8000           0.0070           12.15m\n",
      "      9000           0.0066            6.12m\n",
      "     10000           0.0062            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0721           74.62m\n",
      "         2           0.0700           72.89m\n",
      "         3           0.0681           70.65m\n",
      "         4           0.0662           67.62m\n",
      "         5           0.0644           66.37m\n",
      "         6           0.0627           65.43m\n",
      "         7           0.0610           64.76m\n",
      "         8           0.0594           64.76m\n",
      "         9           0.0579           65.74m\n",
      "        10           0.0564           66.16m\n",
      "        20           0.0444           65.52m\n",
      "        30           0.0362           64.56m\n",
      "        40           0.0305           63.28m\n",
      "        50           0.0265           62.48m\n",
      "        60           0.0237           62.23m\n",
      "        70           0.0217           61.93m\n",
      "        80           0.0203           61.72m\n",
      "        90           0.0192           61.62m\n",
      "       100           0.0185           61.43m\n",
      "       200           0.0159           60.31m\n",
      "       300           0.0150           59.37m\n",
      "       400           0.0145           58.53m\n",
      "       500           0.0140           58.27m\n",
      "       600           0.0136           57.59m\n",
      "       700           0.0133           56.80m\n",
      "       800           0.0131           56.16m\n",
      "       900           0.0128           55.55m\n",
      "      1000           0.0126           54.96m\n",
      "      2000           0.0109           47.79m\n",
      "      3000           0.0098           41.54m\n",
      "      4000           0.0090           35.46m\n",
      "      5000           0.0084           29.47m\n",
      "      6000           0.0078           23.54m\n",
      "      7000           0.0073           17.62m\n",
      "      8000           0.0068           11.72m\n",
      "      9000           0.0064            5.86m\n",
      "     10000           0.0060            0.00s\n"
     ]
    }
   ],
   "source": [
    "for emb in EMB_TYPE[1:]:\n",
    "    X = pd.read_csv(f\"./data/GBRT/{emb}_train.csv\")\n",
    "    X_train, y_train = X.drop(columns=['y']), X['y'].to_numpy()\n",
    "    model = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,\n",
    "                                      max_depth=4, random_state=0, verbose=True)\n",
    "    model.fit(X_train.to_numpy(), y_train)\n",
    "    save_model(model, f\"./data/GBRT/{emb}_trained.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45de09fcf3bd2b79f524a4bd8f7de9946892c46adb40c2fbf980884cdce71d86"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('NED')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
